{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Regression with CAPM\n",
    "\n",
    "## REMAKE THIS, ATOMIZE INTO\n",
    "- Accessing the Data, \n",
    "\n",
    "This code is for making a simple CAPM Regression Model using Dr. French's Historical Data and the TD Ameritrade API's Candle Data\n",
    "\n",
    "It takes data from: \n",
    "- TD Ameritrade and tda-api: https://developer.tdameritrade.com/ & https://tda-api.readthedocs.io/en/latest/\n",
    "- Dr. Kenneth R. French's Data Library(for risk-free rates and market rates): https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html\n",
    "\n",
    "This notebook also utilizes some prewritten functions to pull data without the big setups required for the tda-api, and can be found in the configs.py file in the functions subfolder\n",
    "\n",
    "The Configs and CSV Data Files will be saved in the repository /Users/dB/.secret/ under the filename 'tda-api-v6.json' and pathname 'ff-research-data'. Their paths are listed here for easier access while I write in this notebook:\n",
    "\n",
    "/Users/dB/.secret/tda-api-v6.json\n",
    "\n",
    "/Users/dB/.secret/ff-research-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Necessary Packages and Add Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports -----------------------------------------------------------------------------\n",
    "import functions_for_notebooks.configs as c\n",
    "from tda.client import Client\n",
    "from tda import auth as a\n",
    "import pandas as pd\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs -----------------------------------------------------------------------------\n",
    "tda_api_json_path = '/Users/dB/.secret/tda-api-v6.json'\n",
    "tda_api_json = c.get_account_data(tda_api_json_path)\n",
    "\n",
    "ff_data_path = '/Users/dB/.secret/ff-research-data/F-F_Research_Data_Factors.CSV'\n",
    "ff_data_weekly_path = '/Users/dB/.secret/ff-research-data/F-F_Research_Data_Factors_weekly.CSV'\n",
    "ff_data_daily_path = '/Users/dB/.secret/ff-research-data/F-F_Research_Data_Factors_daily.csv'\n",
    "\n",
    "start_datetime = '01/01/2019' # Collect Data from the Last 5 Years\n",
    "\n",
    "symbol = 'AMD'\n",
    "end = dt.datetime.now() # Today's Date\n",
    "start = end - dt.timedelta(days = 1825) # Subtract 5 Years from Today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Collect Data and Process into DataFrames\n",
    "\n",
    "To collect the data, we need to read Dr. French's CSVs for data and call the TD Ameritrade API for candle data.\n",
    "\n",
    "Reading Dr. French's CSVs is a simple process, all we need to do is decide to use daily or weekly data and then call Panda's read_from_csv() function. We will need to translate the dates into datetime objects to match up with the API data [ADD LATER]\n",
    "\n",
    "Calling the API will pull down candle data for a stock. This will provide us with the open, close, high, low and volume data for the specified timeframe. Since Dr. French's Data only has daily and weekly entries for the data, we should only call tda-api's Client.get_price_history_every_day() or Client.get_price_history_every_week() functions so the dates match. \n",
    "\n",
    "I have a function from the dBI project named get_price_history_df() that can call the API once a Client object is created, and we can pass in '1d' and '1w' as the periods parameter for the daily and weekly data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily Data:\n",
      "                     Mkt-RF   SMB   HML     RF\n",
      "datetime                                      \n",
      "1926-07-01 06:00:00    0.10 -0.25 -0.27  0.009\n",
      "1926-07-02 06:00:00    0.45 -0.33 -0.06  0.009\n",
      "1926-07-06 06:00:00    0.17  0.30 -0.39  0.009\n",
      "1926-07-07 06:00:00    0.09 -0.58  0.02  0.009\n",
      "1926-07-08 06:00:00    0.21 -0.38  0.19  0.009\n",
      "\n",
      "Weekly Data:\n",
      "                     Mkt-RF   SMB   HML     RF\n",
      "datetime                                      \n",
      "1926-07-02 06:00:00    1.60 -0.62 -0.83  0.056\n",
      "1926-07-10 06:00:00    0.36 -0.88  0.31  0.056\n",
      "1926-07-17 06:00:00    1.01  0.59 -1.44  0.056\n",
      "1926-07-24 06:00:00   -2.05  0.10 -0.18  0.056\n",
      "1926-07-31 06:00:00    3.04 -1.82 -0.90  0.056\n"
     ]
    }
   ],
   "source": [
    "# Get the Data from the CSV\n",
    "daily_data = c.read_ff_csvs(ff_data_daily_path)\n",
    "weekly_data = c.read_ff_csvs(ff_data_weekly_path)\n",
    "\n",
    "daily_data.index.name = 'datetime'\n",
    "weekly_data.index.name = 'datetime'\n",
    "\n",
    "# Print the Data\n",
    "print('Daily Data:')\n",
    "print(daily_data.head())\n",
    "\n",
    "print('\\nWeekly Data:')\n",
    "print(weekly_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily Candles: \n",
      "                      open   high    low  close    volume\n",
      "datetime                                                 \n",
      "2019-01-14 06:00:00  19.96  20.62  19.75  20.23  71350248\n",
      "2019-01-15 06:00:00  20.44  20.68  20.26  20.38  62785800\n",
      "2019-01-16 06:00:00  20.40  20.54  19.71  19.73  70849266\n",
      "2019-01-17 06:00:00  19.49  20.51  19.02  20.25  85018421\n",
      "2019-01-18 06:00:00  20.37  21.05  20.02  20.77  88130950\n",
      "\n",
      "Weekly Candles: \n",
      "                       open   high    low  close     volume\n",
      "datetime                                                   \n",
      "2019-01-14 06:00:00  19.960  21.05  19.02  20.77  378134685\n",
      "2019-01-21 06:00:00  20.480  22.03  19.55  21.93  364313502\n",
      "2019-01-28 06:00:00  20.315  25.14  19.05  24.51  765719550\n",
      "2019-02-04 06:00:00  24.430  24.66  22.27  23.05  436607483\n",
      "2019-02-11 06:00:00  23.050  24.05  22.59  23.68  328803629\n"
     ]
    }
   ],
   "source": [
    "# Get our API Parameters for the Candle Data Call\n",
    "ak = tda_api_json['api_key']\n",
    "ru = tda_api_json['redirect_uri']\n",
    "tp = tda_api_json['token_path']\n",
    "\n",
    "# Create the tda.Client object to call the API\n",
    "client = c.connect_to_api(ak, ru, tp)\n",
    "\n",
    "# Call the API for candle data\n",
    "candles_daily = c.get_candles_as_df(c=client, symbol=symbol, periods='1d', start=start, end=end)\n",
    "candles_weekly = c.get_candles_as_df(c=client, symbol=symbol, periods='1w', start=start, end=end)\n",
    "\n",
    "# Printing Candles DataFrames:\n",
    "print('Daily Candles: ')\n",
    "print(candles_daily.head())\n",
    "print('\\nWeekly Candles: ')\n",
    "print(candles_weekly.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Merge the DataFrames\n",
    "\n",
    "This step involves performing an inner-join on the API Data and French's Data DataFrames using DataFrame.concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Mkt-RF   SMB   HML     RF\n",
      "datetime                                      \n",
      "1926-07-01 06:00:00    0.10 -0.25 -0.27  0.009\n",
      "1926-07-02 06:00:00    0.45 -0.33 -0.06  0.009\n",
      "1926-07-06 06:00:00    0.17  0.30 -0.39  0.009\n",
      "1926-07-07 06:00:00    0.09 -0.58  0.02  0.009\n",
      "1926-07-08 06:00:00    0.21 -0.38  0.19  0.009\n",
      "Daily DataFrame:\n",
      "                     open  high  low  close  volume  Mkt-RF   SMB   HML     RF\n",
      "datetime                                                                      \n",
      "1926-07-01 06:00:00   NaN   NaN  NaN    NaN     NaN    0.10 -0.25 -0.27  0.009\n",
      "1926-07-02 06:00:00   NaN   NaN  NaN    NaN     NaN    0.45 -0.33 -0.06  0.009\n",
      "1926-07-06 06:00:00   NaN   NaN  NaN    NaN     NaN    0.17  0.30 -0.39  0.009\n",
      "1926-07-07 06:00:00   NaN   NaN  NaN    NaN     NaN    0.09 -0.58  0.02  0.009\n",
      "1926-07-08 06:00:00   NaN   NaN  NaN    NaN     NaN    0.21 -0.38  0.19  0.009\n",
      "\n",
      "Weekly DataFrame:\n",
      "Empty DataFrame\n",
      "Columns: [open, high, low, close, volume, Mkt-RF, SMB, HML, RF]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the DataFrames\n",
    "print(daily_data.head())\n",
    "\n",
    "daily_df = pd.concat([candles_daily, daily_data], axis=1)\n",
    "weekly_df = pd.concat([candles_weekly, weekly_data], axis=1)\n",
    "weekly_df = weekly_df.dropna()\n",
    "\n",
    "# Print the DataFrames\n",
    "print('Daily DataFrame:')\n",
    "print(daily_df.head())\n",
    "print('\\nWeekly DataFrame:')\n",
    "print(weekly_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get help from Profs:\n",
    "- Data is not recent, and how do I concat these and not get an empty dataframe?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dBI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
